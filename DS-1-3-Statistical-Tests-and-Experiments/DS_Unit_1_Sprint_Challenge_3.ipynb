{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS_Unit_1_Sprint_Challenge_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tclack88/Lambda/blob/master/DS-1-3-Statistical-Tests-and-Experiments/DS_Unit_1_Sprint_Challenge_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NooAiTdnafkz",
        "colab_type": "text"
      },
      "source": [
        "# Data Science Unit 1 Sprint Challenge 3\n",
        "\n",
        "## Exploring Data, Testing Hypotheses\n",
        "\n",
        "In this sprint challenge you will look at a dataset of people being approved or rejected for credit.\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Credit+Approval\n",
        "\n",
        "Data Set Information: This file concerns credit card applications. All attribute names and values have been changed to meaningless symbols to protect confidentiality of the data. This dataset is interesting because there is a good mix of attributes -- continuous, nominal with small numbers of values, and nominal with larger numbers of values. There are also a few missing values.\n",
        "\n",
        "Attribute Information:\n",
        "- A1: b, a.\n",
        "- A2: continuous.\n",
        "- A3: continuous.\n",
        "- A4: u, y, l, t.\n",
        "- A5: g, p, gg.\n",
        "- A6: c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff.\n",
        "- A7: v, h, bb, j, n, z, dd, ff, o.\n",
        "- A8: continuous.\n",
        "- A9: t, f.\n",
        "- A10: t, f.\n",
        "- A11: continuous.\n",
        "- A12: t, f.\n",
        "- A13: g, p, s.\n",
        "- A14: continuous.\n",
        "- A15: continuous.\n",
        "- A16: +,- (class attribute)\n",
        "\n",
        "Yes, most of that doesn't mean anything. A16 is a variable that indicates whether or not a person's request for credit has been approved or denied. This is a good candidate for a y variable since we might want to use the other features to predict this one. The remaining variables have been obfuscated for privacy - a challenge you may have to deal with in your data science career.\n",
        "\n",
        "Sprint challenges are evaluated based on satisfactory completion of each part. It is suggested you work through it in order, getting each aspect reasonably working, before trying to deeply explore, iterate, or refine any given step. Once you get to the end, if you want to go back and improve things, go for it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wch6ksCbJtZ",
        "colab_type": "text"
      },
      "source": [
        "## Part 1 - Load and validate the data\n",
        "\n",
        "- Load the data as a `pandas` data frame.\n",
        "- Validate that it has the appropriate number of observations (you can check the raw file, and also read the dataset description from UCI).\n",
        "- UCI says there should be missing data - check, and if necessary change the data so pandas recognizes it as na\n",
        "- Make sure that the loaded features are of the types described above (continuous values should be treated as float), and correct as necessary\n",
        "\n",
        "This is review, but skills that you'll use at the start of any data exploration. Further, you may have to do some investigation to figure out which file to load from - that is part of the puzzle.\n",
        "\n",
        "Hint: If a column has the datatype of \"object\" even though it's made up of float or integer values, you can coerce it to act as a numeric column by using the `pd.to_numeric()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69v_frheOQgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q79xDLckzibS",
        "colab_type": "code",
        "outputId": "9063e2d5-a3d0-4a74-81f2-d2cf1a85622a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "labels = [\"A\"+str(i) for i in range(1,17)]  # create header names\n",
        "\n",
        "data = \"https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\"\n",
        "df = pd.read_csv(data,names = labels,na_values=\"?\")\n",
        "\n",
        "#df.isna().sum()  # returns 0 but source tells us there are missing values, thus there is a non-standard encoding for nan values\n",
        "#df.A1.unique()   # a '?' is present, this must be the missing value, so we add na_values=\"?\"\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A1</th>\n",
              "      <th>A2</th>\n",
              "      <th>A3</th>\n",
              "      <th>A4</th>\n",
              "      <th>A5</th>\n",
              "      <th>A6</th>\n",
              "      <th>A7</th>\n",
              "      <th>A8</th>\n",
              "      <th>A9</th>\n",
              "      <th>A10</th>\n",
              "      <th>A11</th>\n",
              "      <th>A12</th>\n",
              "      <th>A13</th>\n",
              "      <th>A14</th>\n",
              "      <th>A15</th>\n",
              "      <th>A16</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b</td>\n",
              "      <td>30.83</td>\n",
              "      <td>0.000</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>1.25</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>1</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>202.0</td>\n",
              "      <td>0</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a</td>\n",
              "      <td>58.67</td>\n",
              "      <td>4.460</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>q</td>\n",
              "      <td>h</td>\n",
              "      <td>3.04</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>6</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>43.0</td>\n",
              "      <td>560</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>24.50</td>\n",
              "      <td>0.500</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>q</td>\n",
              "      <td>h</td>\n",
              "      <td>1.50</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>0</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>280.0</td>\n",
              "      <td>824</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b</td>\n",
              "      <td>27.83</td>\n",
              "      <td>1.540</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>3.75</td>\n",
              "      <td>t</td>\n",
              "      <td>t</td>\n",
              "      <td>5</td>\n",
              "      <td>t</td>\n",
              "      <td>g</td>\n",
              "      <td>100.0</td>\n",
              "      <td>3</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b</td>\n",
              "      <td>20.17</td>\n",
              "      <td>5.625</td>\n",
              "      <td>u</td>\n",
              "      <td>g</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>1.71</td>\n",
              "      <td>t</td>\n",
              "      <td>f</td>\n",
              "      <td>0</td>\n",
              "      <td>f</td>\n",
              "      <td>s</td>\n",
              "      <td>120.0</td>\n",
              "      <td>0</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  A1     A2     A3 A4 A5 A6 A7    A8 A9 A10  A11 A12 A13    A14  A15 A16\n",
              "0  b  30.83  0.000  u  g  w  v  1.25  t   t    1   f   g  202.0    0   +\n",
              "1  a  58.67  4.460  u  g  q  h  3.04  t   t    6   f   g   43.0  560   +\n",
              "2  a  24.50  0.500  u  g  q  h  1.50  t   f    0   f   g  280.0  824   +\n",
              "3  b  27.83  1.540  u  g  w  v  3.75  t   t    5   t   g  100.0    3   +\n",
              "4  b  20.17  5.625  u  g  w  v  1.71  t   f    0   f   s  120.0    0   +"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7rLytbrO38L",
        "colab_type": "text"
      },
      "source": [
        "## Part 2 - Exploring data, Testing hypotheses\n",
        "\n",
        "The only thing we really know about this data is that A16 is the class label. Besides that, we have 6 continuous (float) features and 9 categorical features.\n",
        "\n",
        "Explore the data: you can use whatever approach (tables, utility functions, visualizations) to get an impression of the distributions and relationships of the variables. In general, your goal is to understand how the features are different when grouped by the two class labels (`+` and `-`).\n",
        "\n",
        "For the 6 continuous features, how are they different when split between the two class labels? Choose two features to run t-tests (again split by class label) - specifically, select one feature that is *extremely* different between the classes, and another feature that is notably less different (though perhaps still \"statistically significantly\" different). You may have to explore more than two features to do this.\n",
        "\n",
        "For the categorical features, explore by creating \"cross tabs\" (aka [contingency tables](https://en.wikipedia.org/wiki/Contingency_table)) between them and the class label, and apply the Chi-squared test to them. [pandas.crosstab](http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html) can create contingency tables, and [scipy.stats.chi2_contingency](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html) can calculate the Chi-squared statistic for them.\n",
        "\n",
        "There are 9 categorical features - as with the t-test, try to find one where the Chi-squared test returns an extreme result (rejecting the null that the data are independent), and one where it is less extreme.\n",
        "\n",
        "**NOTE** - \"less extreme\" just means smaller test statistic/larger p-value. Even the least extreme differences may be strongly statistically significant.\n",
        "\n",
        "Your *main* goal is the hypothesis tests, so don't spend too much time on the exploration/visualization piece. That is just a means to an end - use simple visualizations, such as boxplots or a scatter matrix (both built in to pandas), to get a feel for the overall distribution of the variables.\n",
        "\n",
        "This is challenging, so manage your time and aim for a baseline of at least running two t-tests and two Chi-squared tests before polishing. And don't forget to answer the questions in part 3, even if your results in this part aren't what you want them to be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0OQjzNWnfYF",
        "colab_type": "text"
      },
      "source": [
        "## T-Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nqcgc0yzm68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# app = approved dataframe\n",
        "# dis = disapproved dataframe\n",
        "app = df[df.A16 == '+']\n",
        "dis = df[df.A16 == '-']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J2KS-8CZWfX",
        "colab_type": "code",
        "outputId": "daf29a4d-7afe-42b4-c63a-8f00e62a0e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "numeric_cols = df.select_dtypes(exclude = 'object').columns.to_list()  # get list of numeric columns\n",
        "\n",
        "def apply_judgement(pvalue,col):\n",
        "  if pvalue > .05:\n",
        "    print(\"\\nNO statistically significant difference between approval rate and\",col)\n",
        "  elif pvalue <.05:\n",
        "    print(\"\\nSTATISTICALLY SIGNIFICANT difference between approval rate and\",col)\n",
        "    \n",
        "    \n",
        "# compare numeric column data    \n",
        "print('Comparing Attributes between approved and dissaproved loans for numeric data')\n",
        "for col in numeric_cols:\n",
        "  print(\"\\n\\n\")\n",
        "  print(col)\n",
        "  t,p = stats.ttest_ind(app[col],dis[col],nan_policy='omit')\n",
        "  print(\"t-statistic:\",t,\"\\tp-value:\",p)\n",
        "  apply_judgement(p,col)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Comparing Attributes between approved and dissaproved loans for numeric data\n",
            "\n",
            "\n",
            "\n",
            "A2\n",
            "t-statistic: 4.2922156166315535 \tp-value: 2.0276637071781407e-05\n",
            "\n",
            "STATISTICALLY SIGNIFICANT difference between approval rate and A2\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "A3\n",
            "t-statistic: 5.52998337614816 \tp-value: 4.551680702308068e-08\n",
            "\n",
            "STATISTICALLY SIGNIFICANT difference between approval rate and A3\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "A8\n",
            "t-statistic: 8.935819983773698 \tp-value: 3.6710537401601785e-18\n",
            "\n",
            "STATISTICALLY SIGNIFICANT difference between approval rate and A8\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "A11\n",
            "t-statistic: 11.667004222431277 \tp-value: 7.957718568079967e-29\n",
            "\n",
            "STATISTICALLY SIGNIFICANT difference between approval rate and A11\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "A14\n",
            "t-statistic: -2.6358251986645476 \tp-value: 0.008586135473979569\n",
            "\n",
            "STATISTICALLY SIGNIFICANT difference between approval rate and A14\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "A15\n",
            "t-statistic: 4.680216020964486 \tp-value: 3.4520256956287944e-06\n",
            "\n",
            "STATISTICALLY SIGNIFICANT difference between approval rate and A15\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY__0gWAnYzv",
        "colab_type": "text"
      },
      "source": [
        "## chi squared tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TbGySZTdQaG",
        "colab_type": "code",
        "outputId": "2b44be8c-acdf-4826-87c1-6b74b57ee14c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "object_cols = df.select_dtypes('object').columns.to_list()\n",
        "object_df = df[object_cols]\n",
        "\n",
        "\n",
        "# Large p-values imply similarity, for there to be distinction in the selection criteria,\n",
        "# there needs to be a small p-value, meaning there's a small chance for these to randomly coincide\n",
        "\n",
        "\n",
        "for col in object_cols[:-1]:\n",
        "  print('\\n\\n\\n',col)\n",
        "  ct = pd.crosstab(object_df[col],object_df.A16)\n",
        "  ct['All'] = ct.sum(axis=1)\n",
        "  ct.loc['All'] = ct.sum()\n",
        "  print(ct)\n",
        "  chi_square,p_value,dof,expected = stats.chi2_contingency(ct)\n",
        "  print(\"chi squared value:\",chi_square,\"\\tpvalue:\",p_value)\n",
        "  apply_judgement(p_value,col)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " A1\n",
            "A16    +    -  All\n",
            "A1                \n",
            "a     98  112  210\n",
            "b    206  262  468\n",
            "All  304  374  678\n",
            "chi squared value: 0.41143508194591916 \tpvalue: 0.981530018361231\n",
            "\n",
            "NO statistically significant difference between approval rate and A1\n",
            "\n",
            "\n",
            "\n",
            " A4\n",
            "A16    +    -  All\n",
            "A4                \n",
            "l      2    0    2\n",
            "u    256  263  519\n",
            "y     45  118  163\n",
            "All  303  381  684\n",
            "chi squared value: 26.234074966202144 \tpvalue: 0.0002013603203375773\n",
            "\n",
            "STATISTICALLY SIGNIFICANT difference between approval rate and A4\n",
            "\n",
            "\n",
            "\n",
            " A5\n",
            "A16    +    -  All\n",
            "A5                \n",
            "g    256  263  519\n",
            "gg     2    0    2\n",
            "p     45  118  163\n",
            "All  303  381  684\n",
            "chi squared value: 26.234074966202144 \tpvalue: 0.0002013603203375773\n",
            "\n",
            "STATISTICALLY SIGNIFICANT difference between approval rate and A5\n",
            "\n",
            "\n",
            "\n",
            " A6\n",
            "A16    +    -  All\n",
            "A6                \n",
            "aa    19   35   54\n",
            "c     62   75  137\n",
            "cc    29   12   41\n",
            "d      7   23   30\n",
            "e     14   11   25\n",
            "ff     7   46   53\n",
            "i     14   45   59\n",
            "j      3    7   10\n",
            "k     14   37   51\n",
            "m     16   22   38\n",
            "q     51   27   78\n",
            "r      2    1    3\n",
            "w     33   31   64\n",
            "x     32    6   38\n",
            "All  303  378  681\n",
            "chi squared value: 98.32520342679135 \tpvalue: 9.446933633611132e-10\n",
            "\n",
            "STATISTICALLY SIGNIFICANT difference between approval rate and A6\n",
            "\n",
            "\n",
            "\n",
            " A7\n",
            "A16    +    -  All\n",
            "A7                \n",
            "bb    25   34   59\n",
            "dd     2    4    6\n",
            "ff     8   49   57\n",
            "h     87   51  138\n",
            "j      3    5    8\n",
            "n      2    2    4\n",
            "o      1    1    2\n",
            "v    169  230  399\n",
            "z      6    2    8\n",
            "All  303  378  681\n",
            "chi squared value: 45.03420714024056 \tpvalue: 0.0004097261730223732\n",
            "\n",
            "STATISTICALLY SIGNIFICANT difference between approval rate and A7\n",
            "\n",
            "\n",
            "\n",
            " A9\n",
            "A16    +    -  All\n",
            "A9                \n",
            "f     23  306  329\n",
            "t    284   77  361\n",
            "All  307  383  690\n",
            "chi squared value: 358.10032647163285 \tpvalue: 3.125328283651242e-76\n",
            "\n",
            "STATISTICALLY SIGNIFICANT difference between approval rate and A9\n",
            "\n",
            "\n",
            "\n",
            " A10\n",
            "A16    +    -  All\n",
            "A10               \n",
            "f     98  297  395\n",
            "t    209   86  295\n",
            "All  307  383  690\n",
            "chi squared value: 144.9276762819937 \tpvalue: 2.485584730494459e-30\n",
            "\n",
            "STATISTICALLY SIGNIFICANT difference between approval rate and A10\n",
            "\n",
            "\n",
            "\n",
            " A12\n",
            "A16    +    -  All\n",
            "A12               \n",
            "f    161  213  374\n",
            "t    146  170  316\n",
            "All  307  383  690\n",
            "chi squared value: 0.6900889348793988 \tpvalue: 0.9525455099582543\n",
            "\n",
            "NO statistically significant difference between approval rate and A12\n",
            "\n",
            "\n",
            "\n",
            " A13\n",
            "A16    +    -  All\n",
            "A13               \n",
            "g    287  338  625\n",
            "p      5    3    8\n",
            "s     15   42   57\n",
            "All  307  383  690\n",
            "chi squared value: 9.191570451545383 \tpvalue: 0.16308747114208172\n",
            "\n",
            "NO statistically significant difference between approval rate and A13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM8JckA2bgnp",
        "colab_type": "text"
      },
      "source": [
        "## Part 3 - Analysis and Interpretation\n",
        "\n",
        "Now that you've looked at the data, answer the following questions:\n",
        "\n",
        "- Interpret and explain the two t-tests you ran - what do they tell you about the relationships between the continuous features you selected and the class labels?\n",
        "- Interpret and explain the two Chi-squared tests you ran - what do they tell you about the relationships between the categorical features you selected and the class labels?\n",
        "- What was the most challenging part of this sprint challenge?\n",
        "\n",
        "Answer with text, but feel free to intersperse example code/results or refer to it from earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIozLDNG2Uhu",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "## My interpretation:\n",
        "Numeric columns were columns: A2, A3, A8, A11, A14, A15 and ALL of them proved to be statistically significant in determining approval rate. This conclusion is based on t-tests and their associated p-values, none of which were above .05 (A14 however was really close, with a p-value of .008, nearly within the top 1% range of its associated t-distribution)\n",
        "\n",
        "Non-Numeric columns were: A1, A4, A5, A6, A7, A9, A10, A12 and A13. In this, A1, A12 and A13 proved to be statstically INSIGNIFICANT in determining loan approval rates. This was determined through chi-squared testing. A1, A12 then A13 with pvalues of .98, .95 and .16 respectively which means the \"probability\" of them appearing similar is rather high. We can see by looking at any of the crosstabs, there are strong similarities in the ratios moving down the columns of the approved(+) and disaproved(-) for A1,A12 and A13.\n",
        "\n",
        "### Conclusion:\n",
        "The features: A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A14, A15 appear to be the best predictors of loan approval. eyeballing the crosstabs, we can see some of those:\n",
        "\n",
        "ex. in A4, y tends to result in loan disapproval\n",
        "\n",
        "The assignment was challenging because there's little to no context with what each category means. Some categories have a \"t\" and \"f\" which may be True and False, but we still can't gather what this may be referring to. This may be helpful in unbiasing the analysis, however it removes some \"common sense checks\" which help vet the analysis. I almost reached the opposite conclusion for the chi-squared test until I realized that low p-values imply low degree of similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krq6QeyrXs6b",
        "colab_type": "text"
      },
      "source": [
        "## Stretch Goals\n",
        "\n",
        "Please do not work on anything listed in this section until you have adequately answered everything in the above three sections, all of these stretch goals are **OPTIONAL** but completing them will ensure you a score of 3 on the corresponding sections.\n",
        "\n",
        "### Section 1 Stretch Goal: (do the following)\n",
        "- Go above and beyond in conducting thorough data exploration including high quality comments and/or markdown text cells to explain your process/discoveries.\n",
        "\n",
        "### Section 2 Stretch Goals: (do one of the following)\n",
        " - Write a function(s) to automate the execution of t-tests on the continuous variables treating the different class labels as the two samples.\n",
        "\n",
        " - Write a funciton(s) to automate the execution of chi^2 tests on all of the different combinations of categorical variables in the dataset.\n",
        "\n",
        "### Section 3 Stretch Goals: (do one of the following)\n",
        "- Construct a confidence interval around the mean of one of the continuous variables, communicate the results/interpretation of that confidence interval in the most consumable/easy-to-understand way that you can. (You may choose to include a graph here, but you don't necessarily have to) Communicate the precision of your estimate as clearly as possible.\n",
        "\n",
        "- Explain the relationship between confidence intervals and T-tests, if possible, use code to demonstrate some important aspects of te"
      ]
    }
  ]
}